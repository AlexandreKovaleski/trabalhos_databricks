Este reposit√≥rio re√∫ne notebooks e scripts desenvolvidos no ambiente Databricks, focados no processamento de grandes volumes de dados (Big Data) e na implementa√ß√£o de pipelines de dados utilizando o Apache Spark.

üöÄ Objetivo do Reposit√≥rio

O prop√≥sito destes trabalhos √© aplicar conceitos de engenharia e ci√™ncia de dados em um ambiente escal√°vel, explorando a integra√ß√£o entre armazenamento em nuvem, processamento distribu√≠do e an√°lise avan√ßada.

üõ†Ô∏è Stack Tecnol√≥gica
Plataforma: Databricks (Community Edition / Enterprise)

Engine de Processamento: Apache Spark

Linguagens: PySpark (Python), SQL

Armazenamento: DBFS (Databricks File System), integra√ß√£o com Delta Lake

üìë Conte√∫do dos Notebooks

Os notebooks presentes neste reposit√≥rio cobrem diversas etapas do ciclo de vida dos dados:

Ingest√£o de Dados: Leitura de diferentes formatos de arquivos (CSV, JSON, Parquet) a partir de sistemas de arquivos distribu√≠dos.

Transforma√ß√£o com PySpark: Uso de DataFrames API para limpeza, filtragem, agrega√ß√µes e jun√ß√µes complexas de dados.

Spark SQL: Execu√ß√£o de consultas otimizadas utilizando sintaxe SQL diretamente sobre os clusters de processamento.

Otimiza√ß√£o de Performance: Aplica√ß√£o de conceitos como caching, partitioning e broadcast joins para melhorar a efici√™ncia dos jobs.

An√°lise de Dados em Escala: Gera√ß√£o de insights e visualiza√ß√µes r√°pidas utilizando as ferramentas nativas de plotagem do Databricks.
